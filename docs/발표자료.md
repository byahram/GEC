# ì‹¤ì „í”„ë¡œì íŠ¸02: ì˜¤íƒ€ ìˆ˜ì • by â€œì˜¤íƒìˆ˜ì •â€

- **íŒ€ì›**: ì˜¤ì •íƒ, ê¹€ì•„ëŒ, ë°•ì¬ì˜, ì´ì„œìœ¨, í™©í˜¸ì„±
- **ê¸°ê°„**: 2025ë…„ 10ì›” 24ì¼(ê¸ˆ) ~ 11ì›” 03ì¼(ì›”)
- **ë°œí‘œì¼**: 2025ë…„ 11ì›” 03ì¼ 9:30 ~

---

## 1. í”„ë¡œì íŠ¸ ê°œìš” ë° ëª©í‘œ (Introduction)

### 1-1. í”„ë¡œì íŠ¸ ê°œìš”

- í…ìŠ¤íŠ¸ ë‚´ì˜ ë¬¸ë²•ì  ì˜¤ë¥˜ë¥¼ ìë™ìœ¼ë¡œ êµì •í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ êµ¬ì¶•
- ì ì ˆí•œ ì‚¬ì „ í•™ìŠµ(\*_Pretrained_) ì–¸ì–´ ëª¨ë¸ì„ íƒìƒ‰í•˜ê³ , ë°ì´í„° íŠ¹ì„±ì— ë§ê²Œ Fine-tunning ì „ëµì„ ìˆ˜ë¦½ ë° ì ìš©í•˜ì—¬ ëª¨ë¸ì˜ ë¬¸ë²• êµì • ì„±ëŠ¥ì„ í–¥ìƒ

### 1-2. í•µì‹¬ ëª©í‘œ

1. **Pretrained ëª¨ë¸ íƒìƒ‰ ë° ì„ ì •**
   - GECì— ì í•©í•œ Transformer ê¸°ë°˜ ëª¨ë¸ ë¹„êµ ë° ì„ ì •
2. **Fine-tunning ì „ëµ ìˆ˜ë¦½ ë° ì‹¤í—˜**
   - ë°ì´í„° ì¦ê°•(Augmentation)ì„ í†µí•œ ì„±ëŠ¥ ê°œì„ 
3. **ë°ì´í„° ê¸°ë°˜ EDA**
   - ë¬¸ì¥ ê¸¸ì´, ì˜¤ë¥˜ ë¹ˆë„, ìˆ˜ì • ë¹„ìœ¨ ë“± í…ìŠ¤íŠ¸ íŠ¹ì„± ë¶„ì„
   - í•™ìŠµ/í‰ê°€ ë°ì´í„° ê°„ ë¶„í¬ ì°¨ì´ íŒŒì•…..?
4. **ì„±ëŠ¥ í‰ê°€ ë° ê°œì„ **
   - BEA ë¦¬ë” ë³´ë“œ ì œì¶œì„ í†µí•œ ê°ê´€ì  ì„±ëŠ¥ ê²€ì¦
5. **í”„ë¡œì íŠ¸ ì‚°ì¶œë¬¼ ì œì‘**
   - ì½”ë“œ, ê²°ê³¼, ë¶„ì„ ê³¼ì •ì„ Githubì— ì •ë¦¬
   - ì‘ë™ í™•ì¸ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì…ë ¥ â†’ ì¶œë ¥ ì‹œì—° ë°ëª¨ ì œì‘

---

## 2. ë°ì´í„° ë¶„ì„ ë° ì „ì²˜ë¦¬ (EDA & Preprocessing)

### 2-1. C4 (í•™ìŠµ)

#### 2-1-1. Sentence-Level Structural Insights

|                                |                                |
| ------------------------------ | ------------------------------ |
| <img src="./image/c4_1.png" /> | <img src="./image/c4_2.png" /> |
| <img src="./image/c4_3.png" /> | <img src="./image/c4_4.png" /> |

<br />

| ê´€ì°°           | ëª¨ë¸ë§ ì¸ì‚¬ì´íŠ¸                                        |
| -------------- | ------------------------------------------------------ |
| ë¬¸ì¥ ê¸¸ì´ ì§§ìŒ | context window â‰¤128 í† í°ìœ¼ë¡œ ì¶©ë¶„                      |
| ì˜ì¡´ ê±°ë¦¬ ê°ì†Œ | Syntax-aware embedding íš¨ê³¼ì                           |
| êµ¬ì¡° ë‹¨ìˆœí™”    | êµ¬ì¡° ë‹¨ìˆœí™” ì •ê·œí™”(simplification regularization) ê°€ëŠ¥ |

#### 2-1-2. Edit Behavior Analysis

|                                |                                |
| ------------------------------ | ------------------------------ |
| <img src="./image/c4_5.png" /> | <img src="./image/c4_6.png" /> |
| <img src="./image/c4_7.png" /> | <img src="./image/c4_8.png" /> |
| <img src="./image/c4_9.png" /> |                                |

<br />

| R   | ì˜ëª»ëœ í† í°ì„ ë‹¤ë¥¸ ê±¸ë¡œ êµì²´ |
| --- | ---------------------------- |
| M   | ëˆ„ë½ëœ í† í° ì‚½ì…             |
| U   | ë¶ˆí•„ìš”í•œ í† í° ì‚­ì œ           |

<br />

| ê´€ì°°             | ëª¨ë¸ë§ ì¸ì‚¬ì´íŠ¸                                                          | ìˆ˜ì¹˜                        |
| ---------------- | ------------------------------------------------------------------------ | --------------------------- |
| ìˆ˜ì •ëŸ‰ ì ìŒ      | Copy-heavy seq2seq(GECToR/PIE) ì´ìƒì                                     | íšŸìˆ˜ : 2~3íšŒ, ë¹„ìœ¨ 10% ë¯¸ë§Œ |
| ì¤‘ê°„ë¶€ ìˆ˜ì • ì§‘ì¤‘ | Attention biasë¥¼ ì¤‘ì•™ êµ¬ê°„ì— ë¶€ì—¬(position embeddingì— ì¤‘ì‹¬ ê°€ì¤‘ì¹˜ ì ìš©) | 0.2~0.6                     |
| Replace ì¤‘ì‹¬     | Substitution-oriented decoder ì„¤ê³„                                       | 70%                         |
| ì² ì ë¹„ì¤‘ ë‚®ìŒ   | Subword ê¸°ë°˜ í† í¬ë‚˜ì´ì € ì í•©                                             | 13.8%                       |

#### 2-1-3. Error Co-occurrence Network

<img src="./image/c4_10.png" />

| ê´€ì°°                         | ëª¨ë¸ë§ ì¸ì‚¬ì´íŠ¸                                    |
| ---------------------------- | -------------------------------------------------- |
| ì˜¤ë¥˜ ìœ í˜• ê°„ ì˜ì¡´            | ë‹¤ì¤‘ ë ˆì´ë¸” ê¸°ë°˜ ì˜ˆì¸¡(Multi-label Edit Prediction) |
| NOUNâ€“VERBâ€“PREPâ€“DET ì—°ê²° ê°•í•¨ | Relation-aware attention ìœ ë¦¬                      |
| ì˜¤ë¥˜ ìŒ ìì£¼ ë“±ì¥            | Co-occurrence ê¸°ë°˜ ë°ì´í„° ì¦ê°• ìœ íš¨                |

#### 2-1-4. Syntactic Simplification

|                                 |                                 |
| ------------------------------- | ------------------------------- |
| <img src="./image/c4_11.png" /> | <img src="./image/c4_12.png" /> |

<br />

| ê´€ì°°              | ëª¨ë¸ë§ ì¸ì‚¬ì´íŠ¸                                                           | ìˆ˜ì¹˜    |
| ----------------- | ------------------------------------------------------------------------- | ------- |
| ì˜ì¡´ ê¹Šì´ ê°ì†Œ    | êµ¬ì¡° ë‹¨ìˆœí™” ìœ ë„ Loss ì„¤ê³„ ê°€ëŠ¥                                           | 10~15%  |
| POS ì—”íŠ¸ë¡œí”¼ ê°ì†Œ | Entropy regularization ì ìš©                                               | 0.3~0.5 |
| êµ¬ì¡° ì•ˆì •í™”       | Over-generation ë°©ì§€ ê¸°ë²• í•„ìš” (í•„ìš” ì´ìƒ ìƒì„±, ê³¼êµì • ë°©ì§€ íŒ¨ë„í‹° ë¶€ì—¬?) |         |

#### 2-1-5. POS Transition Patterns

|                                 |                                 |
| ------------------------------- | ------------------------------- |
| <img src="./image/c4_13.png" /> | <img src="./image/c4_14.png" /> |

<br />

- ì´ì „ í’ˆì‚¬ â†’ í˜„ì¬í’ˆì‚¬
- í˜„ì¬ í’ˆì‚¬ â†’ ì´ì „ í’ˆì‚¬

| ê´€ì°°                             | ëª¨ë¸ë§ ì¸ì‚¬ì´íŠ¸                                      |
| -------------------------------- | ---------------------------------------------------- |
| ì•ˆì •ëœ í’ˆì‚¬ ì „ì´(ì…ë ¥ íŠ¹ì§• ê°•í™”) | POS Embedding + Position Encoding ê²°í•© ìœ íš¨          |
| ì–‘ë°©í–¥ ì¼ê´€ì„±                    | BERT/T5 ê¸°ë°˜ ì–‘ë°©í–¥ attention ìœ ë¦¬                   |
| í•™ìŠµ ì»¤ë¦¬í˜ëŸ¼                    | í’ˆì‚¬ ì•ˆì •ì„± ë†’ì€ ë¬¸ì¥ â†’ ë‚®ì€ ë¬¸ì¥ ìˆœìœ¼ë¡œ ë‹¨ê³„ì  í•™ìŠµ |

#### 2-1-6. Sentence Complexity Features

**ì§€í‘œ ë³€í™” (êµì • ì „â†’í›„)**

| **Feature**       | **ë³€í™”** | **ì˜ë¯¸**         |
| ----------------- | -------- | ---------------- |
| avg_dep_dist      | â†“        | ë¬¸ì¥ êµ¬ì¡° ë‹¨ìˆœí™” |
| pos_entropy_tri   | â†“        | í’ˆì‚¬ ë‹¤ì–‘ì„± ê°ì†Œ |
| edit_ratio_tokens | â‰ˆ0.1     | ì†Œê·œëª¨ ìˆ˜ì •      |
| clause_count      | â†“        | ì ˆ ê°œìˆ˜ ê°ì†Œ     |
| verb_ratio        | â‰ˆ ë™ì¼   | ì˜ë¯¸ ë³´ì¡´        |

#### 2-1-7. Feature Correlation (Spearman)

**ìƒê´€ ë¶„ì„ ê²°ê³¼**

| **Feature Pair**             | **Ï** | **í•´ì„**                    |
| ---------------------------- | ----- | --------------------------- |
| pos_entropy_tri â†” edit_ratio | +0.62 | POS ë‹¤ì–‘ì„±â†‘ â†’ êµì •ëŸ‰â†‘       |
| avg_dep_dist â†” edit_ratio    | +0.45 | ë¬¸ë²• ë³µì¡ë„â†‘ â†’ ì˜¤ë¥˜ ê°€ëŠ¥ì„±â†‘ |

#### [C4_EDA ê²°ê³¼](https://www.notion.so/C4_EDA-29965bf8ceca8041bd06c1bec159dc41)

### 2-2. BEA19 (í‰ê°€)

| text ë‹¨ì–´ ìˆ˜                      | edit ê°œìˆ˜                         |
| --------------------------------- | --------------------------------- |
| <img src="./image/bea19_1.png" /> | <img src="./image/bea19_2.png" /> |

<br />

| len_words | **count** | **mean**  | **std**   | **min** | **25%** | **50%** | **75%** | **max** |
| --------- | --------- | --------- | --------- | ------- | ------- | ------- | ------- | ------- |
| **BEA**   | 2856.0    | 22.658964 | 13.244644 | 2.0     | 14.0    | 20.0    | 28.0    | 157.0   |

<br />

| len_chars | **count** | **mean**   | **std**   | **min** | **25%** | **50%** | **75%** | **max** |
| --------- | --------- | ---------- | --------- | ------- | ------- | ------- | ------- | ------- |
| BEA       | 2856.0    | 114.562325 | 69.170134 | 8.0     | 69.0    | 102.0   | 143.0   | 790.0   |

<br />

| **C4_rows**       | **BEA_rows**       | **C4_avg_words**    | **BEA_avg_words** |
| ----------------- | ------------------ | ------------------- | ----------------- |
| 30000             | 2854               | 21.9422             | 22.660477         |
| **BEA_edit_mean** | **C4_POS_entropy** | **BEA_POS_entropy** | **C4_edit_mean**  |
| 0.088296          | 4.286405           | 4.335439            | 0.147281          |

#### [EDA ê²°ê³¼](https://www.notion.so/EDA-296e51228f4b808cb2aac129f2ce93f7)

---

## 3. ëª¨ë¸ êµ¬í˜„

### 3-0. ë©¤ë²„ ë³„ ëª¨ë¸ êµ¬í˜„

### 3-1. ë°ì´í„° ì¤€ë¹„

- [ì˜¤ë¥˜ê°€ ì„ì¸ ë¬¸ì¥, ì •ë‹µ ë¬¸ì¥]

<img src="./ë°œí‘œìë£Œ/image/data_1.png" />

- GEC_utill ì‚¬ìš© â†’ [í† í°, íƒœê·¸]

```jsx
config
[í† í°]SEPL|||SEPR[íƒœê·¸]

precessed.dry :
$STARTSEPL|||SEPR$KEEP MuchSEPL|||SEPR$DELETE manySEPL|||SEPR$TRANSFORM_CASE_CAPITAL brandsSEPL|||SEPR$KEEP andSEPL|||SEPR$KEEP sellersSEPL|||SEPR$KEEP stillSEPL|||SEPR$KEEP inSEPL|||SEPR$KEEP theSEPL|||SEPR$KEEP market.SEPL|||SEPR$KEEP
$STARTSEPL|||SEPR$KEEP FairySEPL|||SEPR$KEEP OrSEPL|||SEPR$KEEP Not,SEPL|||SEPR$KEEP I'mSEPL|||SEPR$KEEP theSEPL|||SEPR$KEEP Godmother:SEPL|||SEPR$KEEP noSEPL|||SEPR$REPLACE_Not justSEPL|||SEPR$APPEND_a look,SEPL|||SEPR$KEEP butSEPL|||SEPR$KEEP mySEPL|||SEPR$KEEP outfitSEPL|||SEPR$KEEP forSEPL|||SEPR$KEEP takingSEPL|||SEPR$APPEND_on theSEPL|||SEPR$KEEP partSEPL|||SEPR$REPLACE_role asSEPL|||SEPR$KEEP godmother.SEPL|||SEPR$KEEP
```

```python
TOKENS : ['he', 'go', 'to', 'school']
TAGS: ['$KEEP', '$TRANSFORM_VERB_VB_VBD', '$KEEP', '$KEEP']
```

```xml
[Line 1]
  Much                  <--  $DELETE
  many                  <--  $TRANSFORM_CASE_CAPITAL
  brands                <--  $KEEP
  and                   <--  $KEEP
  sellers               <--  $KEEP
  still                 <--  $KEEP
  in                    <--  $KEEP
  the                   <--  $KEEP
  market.               <--  $KEEP
[Line 2]
  Fairy                 <--  $KEEP
  Or                    <--  $KEEP
  Not,                  <--  $KEEP
  I'm                   <--  $KEEP
  the                   <--  $KEEP
  Godmother:            <--  $KEEP
  no                    <--  $REPLACE_Not
  just                  <--  $APPEND_a
  look,                 <--  $KEEP
  but                   <--  $KEEP
  my                    <--  $KEEP
  outfit                <--  $KEEP
  for                   <--  $KEEP
  taking                <--  $APPEND_on
  the                   <--  $KEEP
  part                  <--  $REPLACE_role
  as                    <--  $KEEP
  godmother.            <--  $KEEP
```

- íƒœê·¸ ì˜ˆì‹œ

| íƒœê·¸ëª…                     | ë™ì‘                             |
| -------------------------- | -------------------------------- |
| KEEP                       | ê·¸ëŒ€ë¡œ ìœ ì§€                      |
| DELETE                     | ì‚­ì œ                             |
| APPEND_the                 | í•´ë‹¹ í† í° ë’¤ì— theë¥¼ ë¶™ì„        |
| REPLACE_in                 | í•´ë‹¹ í† í°ì„ inìœ¼ë¡œ êµì²´          |
| TRANSFORM_CASE_CAPITAL     | í•´ë‹¹ í† í° ì²«ê¸€ìë¥¼ ëŒ€ë¬¸ìë¡œ êµì²´ |
| TRANSFORM_VERB_VB_VBZ      | ë™ì‚¬ì˜ ì‹œì œ,ìˆ˜ì¼ì¹˜ ë³€í™˜          |
| TRANSFORM_AGREEMENT_PLURAL | ëª…ì‚¬ì˜ ìˆ˜ì¼ì¹˜ ë³€í™˜               |

- verb_from_vocab

```
{
...
go_goes:VB_VBZ
go_going:VB_VBG
go_gone:VB_VBN
go_went:VB_VBD
...
}
```

### 3-2. í† í¬ë‚˜ì´ì¦ˆ

#### byte-level BPE

```xml
she is teacher.
<s> Ä She Ä is Ä teach er . </s>
-100, KEEP, KEEP, APPEND_A_ID, -100, KEEP, -100
```

- ìŠ¤í˜ì…œ í† í° / íŒ¨ë”©ì€ -100ìœ¼ë¡œ í•™ìŠµ ì‹œ ì†ì‹¤ì—ì„œ ì œì™¸
- ë¬¸ì¥ ê¸¸ì´ê°€ ê¸´ ê²½ìš° ë¼ë²¨ë„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì˜ë¦¼
  - MAX_LEN ì„¤ì •ì‹œ ê³ ë ¤
- ë¯¸ì§€ íƒœê·¸ëŠ” KEEPìœ¼ë¡œ ì²˜ë¦¬

#### vocab-size

- RoBERTa-base = 50265
- LABEL2ID = 5002

```json
LABEL2ID.json
{
  "$KEEP": 0,
  "$DELETE": 1,
  "$TRANSFORM_CASE_CAPITAL": 2,
  "$APPEND_the": 3,
  ...
  "$REPLACE_severely": 4998,
  "$REPLACE_ages": 4999,
  "@@UNKNOWN@@": 5000,
  "@@PADDING@@": 5001
}
```

### 3-3. Pretrained(ë°±ë³¸)

- **ë°±ë³¸ ì•„í‚¤í…ì²˜:** **RoBERTa-base ê³„ì—´**
  - **ì„ë² ë”© ì°¨ì›(hidden size):** 768
  - **ì¸ì½”ë” ì¸µ ìˆ˜:** 12 (RobertaLayer Ã— 12)
  - **FFN(intermediate) ì°¨ì›:** 3,072
  - **ì–´í…ì…˜:** Multi-head self-attention (RoBERTa-baseëŠ” í†µìƒ 12-head)
  - **í¬ì§€ì…˜ ì„ë² ë”© ê¸¸ì´:** 514
  - **ì–´íœ˜ ì§‘í•© í¬ê¸°:** 50,265
  - **í† í° íƒ€ì… ì„ë² ë”©:** í¬ê¸° 1 (RoBERTaëŠ” segment êµ¬ë¶„ì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ dummy)
  - **LayerNorm/Dropout:** í‘œì¤€ RoBERTa ì„¤ì • (eps=1e-5, p=0.1)

### 3-4. Fine-tuning í—¤ë“œ(Task ì „ìš©)

- **í—¤ë“œ í´ë˜ìŠ¤:** `RobertaForTokenClassification`
- **í—¤ë“œ êµ¬ì„±:**
  - `Dropout(p=0.1)`
  - `Linear(in_features=768, out_features=5002, bias=True)`
- **íƒœìŠ¤í¬ ìœ í˜•:** **Token Classification** (ì‹œí€€ìŠ¤ì˜ ê° í† í°ì— ë¼ë²¨ ë¶€ì—¬)
- **ë¼ë²¨ ìˆ˜:** **5002 í´ë˜ìŠ¤**
  - ë§¤ìš° í° ë¼ë²¨ ê³µê°„ â†’ íƒœê·¸ ì„¸íŠ¸ê°€ ë°©ëŒ€í•¨ (ì˜ˆ: ì„¸ë¶„í™”ëœ ì˜¤ë¥˜ìœ í˜• íƒœê¹… ë“±)

### 3-5. íŒŒì´í”„ë¼ì¸ ê´€ì (ì…Â·ì¶œë ¥ê³¼ ì†ì‹¤)

- **ì…ë ¥:** `input_ids`, `attention_mask`, `labels`
  ```jsx
  DatasetDict({
      train: Dataset({
          features: ['input_ids', 'attention_mask', 'labels'],
          num_rows: 199000
      })
      validation: Dataset({
          features: ['input_ids', 'attention_mask', 'labels'],
          num_rows: 1000
      })
  })
  ```
- **ì¶œë ¥:** í† í°ë³„ ë¡œì§“ (shape: batch Ã— MAX_LEN Ã— 5002)
- **ì†ì‹¤:** `CrossEntropyLoss`(ë¼ë²¨ì´ ì£¼ì–´ì§€ë©´ ìë™ ê³„ì‚°, `ignore_index`ë¡œ íŒ¨ë”© í† í° ë¬´ì‹œ)

### 3-6. ì‹¤í—˜

- RoBERTa ê°€ì¤‘ì¹˜ ë™ê²°(A) vs ë¹„ë™ê²°(B)
  - ì´ˆê¸° ì†ë„ëŠ” Aê°€ í™•ì‹¤íˆ ë¹ ë¦„
  - ìµœì¢… ì„±ëŠ¥ì€ Bê°€ ìš°ì„¸
  - ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…ìœ¼ë¡œëŠ” A ì‚¬ìš©, ìµœì¢… í’ˆì§ˆ/ì¼ë°˜í™”ëŠ” B ì‚¬ìš©
  - ì ì§„ì  unfreeze + ì°¨ë“± LR ì ìš©
  - ì¶”ì  ê²°ê³¼
    ![W&B Chart 2025. 10. 31. ì˜¤ì „ 11_42_44.png](attachment:f021493d-d1c5-4b54-a355-bd46a8debd4e:WB_Chart_2025._10._31._ì˜¤ì „_11_42_44.png)
    ![W&B Chart 2025. 10. 31. ì˜¤ì „ 11_42_21.png](attachment:07fabab2-6aad-4c86-aebb-0e5523e1e30f:WB_Chart_2025._10._31._ì˜¤ì „_11_42_21.png)
    ![W&B Chart 2025. 10. 31. ì˜¤ì „ 11_40_10.png](attachment:812344b3-dbd8-46e6-846c-a03c5615cad7:WB_Chart_2025._10._31._ì˜¤ì „_11_40_10.png)
    ![W&B Chart 2025. 10. 31. ì˜¤ì „ 11_39_54.png](attachment:4a876da7-4748-4f6f-8d71-2a72c8e0ce01:WB_Chart_2025._10._31._ì˜¤ì „_11_39_54.png)
- LoRA ì‚¬ìš©(A) vs ë¯¸ì‚¬ìš©(B)
  - Aì˜ ê²½ìš° ì†ì‹¤ í•˜í•œì´ ë†’ê²Œ ê³ ì •
  - í‘œí˜„ ì¡°ì •ì˜ ììœ ë„ê°€ ë¶€ì¡±í•´ ì •ë°€ë„ê°€ ì¶©ë¶„íˆ ìƒìŠ¹í•˜ì§€ ëª»í–ˆìŒ
  - íƒ€ê²Ÿ ëª¨ë“ˆ í™•ì¥(FFN) í•„ìš”
  - ì¶”ì  ê²°ê³¼
    ![W&B Chart 2025. 10. 31. ì˜¤í›„ 12_03_56.png](attachment:8b398862-98d3-4d7e-8bda-b4f1679c2c64:WB_Chart_2025._10._31._ì˜¤í›„_12_03_56.png)
    ![W&B Chart 2025. 10. 31. ì˜¤í›„ 12_03_10.png](attachment:faacab8f-64f2-4aee-b335-a2194d6fcb98:WB_Chart_2025._10._31._ì˜¤í›„_12_03_10.png)
- ì¶”ê°€í•™ìŠµ HardSampling ë°©ì‹
  - í•™ìŠµA = ì˜¤ë‹µë¥ ì´ ë†’ì€ ìƒìœ„ íƒœê·¸ë“¤ì´ í¬í•¨ëœ ìƒ˜í”Œ 20K
  - í•™ìŠµB = ì˜¤ë‹µë¥ ì´ ë†’ì€ ìƒìœ„ íƒœê·¸ë“¤ì´ í¬í•¨ëœ ìƒ˜í”Œ 10K + ì¼ë°˜ ìƒ˜í”Œ 10K
  - Aê°€ ê³¼ë„í•˜ê²Œ í•™ìŠµë˜ëŠ” ê²½í–¥ì„ í™•ì¸í•˜ê³  Bí•™ìŠµìœ¼ë¡œ ì§„í–‰
  - ì¶”ì ê²°ê³¼
    ![W&B Chart 2025. 10. 31. ì˜¤í›„ 4_15_28.png](attachment:bca3bf0b-0666-4fce-a032-50d68b894df5:WB_Chart_2025._10._31._ì˜¤í›„_4_15_28.png)
- ì¶”ê°€í•™ìŠµ ClassWeight ì ìš©
  - ê³¼ë„í•œ KEEP ì˜ˆì¸¡ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ClassWeight ì ìš©
  - `smoothing`: **ë¼í”Œë¼ìŠ¤ ë³´ì •** ë¹„ìŠ·í•œ ì—­í• . 0ì´ë©´ ê·¹ë‹¨ ê°€ì¤‘ì¹˜ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆì–´ ìœ„í—˜.
    - ê° í´ë˜ìŠ¤ countì— `smoothing * (total/num_labels)` ì •ë„ë¥¼ ë”í•´ **0 ë¶„ëª¨**/**ê·¹ë‹¨ê°’** ë°©ì§€.
    - `smoothing`ì„ í‚¤ìš°ë©´ ì „ì²´ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì°¨ì´ê°€ ì™„ë§Œí•´ì ¸ **ì•ˆì •ì„±â†‘**(ë„ˆë¬´ ê³¼ê²©í•œ ë¦¬ë°¸ëŸ°ì‹± ë°©ì§€).
  - `power`: **ì—­ë¹ˆë„(inverse-frequency) ì§€ìˆ˜**.
    - `power`ë¥¼ ì˜¬ë¦¬ë©´ í¬ì†Œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ê°€ ë” ì»¤ì ¸ **FNâ†“**(ë¦¬ì½œâ†‘) ê²½í–¥, ê·¸ëŸ¬ë‚˜ **FPâ†‘** ê°€ëŠ¥ì„±ë„ í•¨ê»˜ ì»¤ì§.
      | parameter | í•™ìŠµA | í•™ìŠµB |
      | --------- | ----- | ----- |
      | smoothing | 0.05 | 0.15 |
      | power | 0.5 | 0.3 |
- ì¶”ì ê²°ê³¼
  <img src="./ë°œí‘œìë£Œ/image/result.png" />

### 3-7. ì¶”ë¡ 

- í‰ê°€ ì§€í‘œ - **BEA 2019 Shared Task - Grammatical Error Correction - All Tracks**
- ë°˜ë³µ ì¶”ë¡ ì„ í†µí•´ì„œ ì •í™•ë„ ìƒìŠ¹
  ```python
  Source_0  : I every day go to school.
  Reference : I go to school every day.
  --- iterative outputs ---
  (r=1) Input     : I every day go to school.
        Hypothesis: I Every day go to school.
        Actions   : ['$KEEP', '$TRANSFORM_CASE_CAPITAL', '$KEEP', '$KEEP', '$KEEP', '$KEEP']
        #Tokens   : 6
  (r=2) Input     : I Every day go to school.
        Hypothesis: Every day go to school.
        Actions   : ['$DELETE', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP']
        #Tokens   : 6
  (r=3) Input     : Every day go to school.
        Hypothesis: Every day go to school.
        Actions   : ['$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP']
        #Tokens   : 5
        -> early stop: unchanged at r=3
  ```
  | ë°˜ë³µ ì¶”ë¡  íšŸìˆ˜ | Score |
  | -------------- | ----- |
  | r = 1          | 45.13 |
  | r = 2          | 45.7  |
  | r = 3          | 45.82 |
- ë³´í†µ 1-2íšŒ ì‚¬ì´ì—ì„œ í° í­ìœ¼ë¡œ ìƒìŠ¹í•˜ê³ , 3íšŒ ì´í›„ì—ëŠ” ìˆ˜ë ´
- ê´€ì‚¬, ì‹œì œ, ìˆ˜ ì¼ì¹˜, ì² ìì²˜ëŸ¼ êµ­ì†Œ ìˆ˜ì •ì´ ê°€ëŠ¥í•œ ì˜¤ë¥˜ëŠ” r=1~2ì—ì„œ ëŒ€ë¶€ë¶„ í•´ê²°
- ì „ì¹˜ì‚¬, ì–´ìˆœ, ì–´íœ˜ ì„ íƒì²˜ëŸ¼ ë¬¸ë§¥/ì˜ì¡´ ì •ë³´ê°€ ë” í•„ìš”í•œ ì˜¤ë¥˜ëŠ” r=2~3ì—ì„œ ì¶”ê°€ ê°œì„ ì´ ìì£¼ ê´€ì°°

#### ì˜¤ë¥˜ ìœ í˜• ë³„ í¬ì¸íŠ¸

| ìœ í˜•                        | ë°˜ë³µì— ë”°ë¥¸ ì „í˜•ì  íŒ¨í„´                | ë¹„ê³                                                      |
| --------------------------- | -------------------------------------- | -------------------------------------------------------- |
| ê´€ì‚¬(article)               | r=1ì—ì„œ ê±°ì˜ í•´ê²°. ì¶”ê°€ ë°˜ë³µ ì´ë“ ë¯¸ë¯¸ | ì˜ˆ: â€œShe is teacher.â€ â†’ r=1ì— â€œaâ€ ì‚½ì…                   |
| ì‹œì œ/ìˆ˜ì¼ì¹˜(verb tense/SVA) | r=1ì—ì„œ í•´ê²°ë˜ê±°ë‚˜ r=2ì—ì„œ ì •êµí™”      | â€œHe go â†’ goesâ€ ë¥˜ëŠ” 1íŒ¨ìŠ¤, ê¸¸ê±°ë‚˜ ë³µí•©êµ¬ì¡°ë©´ r=2         |
| ì² ìÂ·í˜•íƒœ(spelling/morph)   | r=1ì—ì„œ ëŒ€ë¶€ë¶„ í•´ê²°                    | ì˜¤íƒ€ê°€ ë‹¤ì„œë¸Œì›Œë“œ ë¶„í•´ë˜ì–´ë„ 1íŒ¨ìŠ¤ ì ìš©                  |
| ì „ì¹˜ì‚¬(prep)                | r=2ì—ì„œ ì¶”ê°€ ê°œì„  ë¹ˆë²ˆ                 | ì²« íŒ¨ìŠ¤ í† í° êµì²´ í›„ ë¬¸ë§¥ì— ë§ì¶˜ ë³´ì •ì´ r=2ì—ì„œ ì´ë£¨ì–´ì§ |
| ì–´ìˆœ(word order)            | r=2 ê°œì„ , r=3 ë¯¸ì„¸ì¡°ì •                 | ì–´ìˆœ ì¬ë°°ì¹˜ í›„ ë¶€ìˆ˜ì  ê´€ì‚¬/ì „ì¹˜ì‚¬ ë³´ì •ì´ ë’¤ë”°ë¦„          |
| ì–´íœ˜ ì„ íƒ(word choice)      | r=2ì—ì„œ ê°œì„  ê´€ì°°, râ‰¥3ì€ ìˆ˜ë ´          | ë™ì˜ì–´/ì˜ë¯¸ ì°¨ì´ë¡œ ì¸í•´ 1íŒ¨ìŠ¤ì— ë§ì„¤ì´ê³  2íŒ¨ìŠ¤ì— í™•ì •    |
|                             |

---

## 4. ë°ëª¨ êµ¬í˜„

### 4-1. êµ¬í˜„ êµ¬ì¡°

#### ì£¼ìš” íŒŒì¼ êµ¬ì„±

```
GEC/
â”œâ”€ [app.py](http://app.py/)                     # Gradio ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€ requirements.txt           # ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ëª©ë¡
â”œâ”€ .gitignore                 # ëŒ€í˜• ëª¨ë¸ íŒŒì¼ ì œì™¸ ì„¤ì •
â”œâ”€ roberta_gector_k5000_noCE_1900K/
â”‚   â”œâ”€ config.json            # ëª¨ë¸ ì„¤ì • íŒŒì¼
â”‚   â”œâ”€ tokenizer.json         # í† í¬ë‚˜ì´ì € ì„¤ì •
â”‚   â”œâ”€ vocab.json / merges.txt# RoBERTa BPE ì‚¬ì „
â”‚   â”œâ”€ gector_utils.py        # êµì • ë¡œì§ ìœ í‹¸
â”‚   â”œâ”€ model.safetensors      # (ëŒ€ìš©ëŸ‰, ì™¸ë¶€ ë§í¬ì—ì„œ ë‹¤ìš´ë¡œë“œ)
â”‚   â””â”€ ...
```

#### Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì¡°

- **ì…ë ¥ ì˜ì—­**: ì‚¬ìš©ìê°€ êµì •í•˜ê³ ì í•˜ëŠ” ë¬¸ì¥ ì…ë ¥
- **ì¶œë ¥ ì˜ì—­**
  - êµì •ëœ ë¬¸ì¥ (ë³µì‚¬ ê°€ëŠ¥)
  - ì…ë ¥ ë¬¸ì¥ê³¼ êµì • ê²°ê³¼ì˜ ì°¨ì´ë¥¼ **í•˜ì´ë¼ì´íŠ¸ ìƒ‰ìƒ**ìœ¼ë¡œ í‘œì‹œ
    - ğŸ”´ **ì˜ëª»ëœ ë‹¨ì–´ (ì‚­ì œ/ë³€ê²½ í•„ìš”)**
    - ğŸŸ¢ **ìˆ˜ì •ëœ ë‹¨ì–´ (êµì²´ëœ ë‹¨ì–´)**

### 4-2. ê²°ê³¼ ìš”ì•½

- RoBERTa ê¸°ë°˜ GEC ëª¨ë¸ì„ í™œìš©í•œ ë¬¸ë²• ì˜¤ë¥˜ ìë™ êµì •
- Gradio UIë¥¼ í†µí•´ ì‹¤ì‹œê°„ ë¬¸ì¥ ì…ë ¥ ë° ê²°ê³¼ ì‹œê°í™”
- ì˜ëª»ëœ ë‹¨ì–´/ìˆ˜ì •ëœ ë‹¨ì–´ í•˜ì´ë¼ì´íŠ¸ í‘œì‹œë¡œ ì§ê´€ì ì¸ ë¹„êµ ê°€ëŠ¥
- ëŒ€í˜• ëª¨ë¸ íŒŒì¼ì€ `.gitignore` ì²˜ë¦¬ ë° ì™¸ë¶€ ë§í¬ë¡œ ê´€ë¦¬

<img src="./ë°œí‘œìë£Œ/image/demo.png" />

### ê° ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½

| ì˜¤ë¥˜ í˜•íƒœ           | ì˜¤ë‹µ ë¬¸ì¥                  | ì •ë‹µ ë¬¸ì¥                    | ëª¨ë¸A                           | ëª¨ë¸B                            | ëª¨ë¸C                            | ëª¨ë¸D                        | ëª¨ë¸E                            |
| ------------------- | -------------------------- | ---------------------------- | ------------------------------- | -------------------------------- | -------------------------------- | ---------------------------- | -------------------------------- |
| ì‹œì œ ë™ì‚¬ í˜•íƒœ ì˜¤ë¥˜ | He go to school every day. | He goes to school every day. | he **gos** to school every day. | He **goes** to school every day. | He **went** to school every day. | He goes to school every day. | He **goes** to school every day. |
| ìˆ˜ì¼ì¹˜ ì˜¤ë¥˜         | She has two child.         | She has two children.        | She has two **child**.          | She has two **children.**        | She has two **children**.        | She has two child.           | She has two **child.**           |

| ê´€ì‚¬ ì˜¤ë¥˜
| She is teacher. | She is a teacher. | She **an** teacher. | She is **a** teacher. | She is **a** teacher. | She is teacher. | She is **a** teacher. |
| ì „ì¹˜ì‚¬ ì˜¤ë¥˜
| He arrived to the airport on time. | He arrived at the airport on time. | He arrived **to** the airport on time. | He arrived **at** the airport on time. | He arrived **at** the airport on time. | He arrived to the airport on time. | He arrived **at** the airport on time. |
| ì–´ìˆœ ì˜¤ë¥˜
| I every day go to school. | I go to school every day. | I **every go to school**. | I **go to school every day.** | I every day go to school. | I every day go to school. | **Every** day go to school. |
| ì–´íœ˜ ì„ íƒ ì˜¤ë¥˜
| He told that he was tired. | He said that he was tired. | He **tolded** that he was tired. | He told **him** that he was tired. | He **knew** that he was tired. | He told that he was tired. | He **said** that he was tired. |
| ì² ì ë° í˜•íƒœ ì˜¤ë¥˜ | This is very importent information. | This is very important information. | This is very **importent** information. | This is very **important** information. | This is very **important** information. | This is very importent information. | This is very **important** information. |
| í‰ê·  ì‹œê°„ | | | 0.0238 sec | 0.351 sec | 0.327 sec | 0.817 sec | 0.014 sec |

---

## 5. Reference

- **GECToR â€“ Grammatical Error Correction: Tag, Not Rewrite (2020)** - https://aclanthology.org/2020.bea-1.16.pdf
- **Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction (2022) -** https://arxiv.org/pdf/2203.13064
